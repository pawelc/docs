#[An auxiliary variational method]()
Adding auxiliary variable y to the model so \sum_y p(x,y)=p(x) then the q(x,y) can be simpler and q(x) is fully connected like in case
of mixtures (y is a label).

# Stochastic Variational Inference
2012

Standard VI cannot cope with large quantities of data because we have to run local variational parameter update for each data point before updating global variational parameter.
Using exponential family for priors, posteriors and likelihood. Also variational approximations are from the same exponential family and factorize.
Open question how to handle it formally in the streaming setting
In nonconjugate (non exponential) setting the variational inference is more difficult.
There is similarity between deep learning and probabilistic models with hidden structure. Both use hidden structure to encode structure in the data. The variational method gives us view on the model that makes computations tractable, for example by using factorial approximation to posterior distribution.
“Variational inference is amenable to stochastic optimization because the variational objective decomposes into a sum of terms, one for each data point in the analysis.”
“Variational inference casts the inference problem as an optimization.”
Variational inference minimizes KL divergence  from the variational distribution to the posterior distribution. It maximizes the evidence lower bound (ELBO) log p(x)
Stochastic gradient uses point from the local context (few in mini batch case) to re estimate local variational parameters and global one. The regular mean field variational approximation has to reestimate all local parameters (for all local contexts) before updating global parameter.
# Latent Dirichlet Allocation
Parametric Bayesian hierarchical model to model document of words generated by the many topics. 
“De Finetti’s representation theorem states that the joint distribution of an infinitely exchangeable sequence of random variables is as if a random parameter were drawn from some distribution and then the random variables in question were independent and identically distributed, conditioned on that parameter.”

# Natural Gradient Works Efficiently in Learning 
·         The parameters of the NN do not lie in the Euclidian space but in the Riemannian space. Therefore the ordinary gradient is not the best direction to move parameter estimate.
·         NN shown as probabilistic model trained using natural gradient
·         Online learning rule is Fisher efficient
·         Derivation of the optimal learning rate

Revisiting Natural Gradient for Deep Networks
·         Compares different methods improving optimization (Hessian Free, Krylov subspace, natural gradient, TONGA)
·         extend natural gradient descent to incorporate second order information
·         natural gradient successfully applied in reinforcement learning and for stochastic search
·         The Fisher Information Matrix form can be obtained from the expected value of the Hessian through algebraic manipulations
·         Natural gradient derivation for NN by treating NN probabilistically
·         Natural gradient can be used in online learning
·         “one can therefore view both Hessian-Free Optimization and KSD as being implementations of natural gradient descent”

https://charlesmartin14.wordpress.com/2015/04/01/why-deep-learning-works-ii-the-renormalization-group/
·         “Deep learning appears to be a real-space variational Renormalization Group technique, specifically applicable to very complex, inhomogeneous systems where the detailed scale transformations have to be learned from the data”
·         “it is possible to directly monitor the overfitting by comparing the free energies of training data and held out validation data…If the model is not overfitting at all, the average free energy should be about the same on training and validation data”

# Black Box Variational Inference
2013
“Maximizing the ELBO is equivalent to minimizing the KL divergence (Jordan et al., 1999; Bishop, 2006). Intuitively, the first term rewards variational distributions that place high mass on configurations of the latent variables that also explain the observations; the second term rewards variational distributions that are entropic, i.e., that maximize uncertainty by spreading their mass on many configurations.”
The expectation with respect to varuational distribution in the ELBO has to be derived for each model in traditional variational apprach.
Gradient of the ELBO can be written as expectation wrt variational distribution so we can use MC samples to get gradient.
The joint in the gradient can be replaced by unnormalized version.
The  gradient ELBO is computed using expected value so we can use the MC. But the variance of such estimator is large and we have to apply variance reduction methods
Uses Rao-Blackwellization and control variates to decrease the variance of gradient estimator (variance reduction methods work by replacing the function whose expectation is being approximated by Monte Carlo with another function that has the same expectation but smaller variance)
Evaluate the models using predictive likelihood

#[Neural Variational Inference and Learning in Belief Networks]()
2014
NVIL - Similar method to Black Box Variational Inference i.e. using REINFORCE estimator for the gradient of the ELBO. It also uses baselines 
(control variates) but the baseline in this case in the neural network that depend on data. Authors also create algorithm
that the leraning signal for parameters depends on the layer hence decreasing the variance further. They also scale the learning 
signal.

#[Automated variational inference in probabilistic programming]()
How to run VI in the probabilistic program. Authors use some kind of RL algorithm with optimization of generic ELBO and descreasing the variance
using the control variates. They test their approach on few models. 

# Bayesian Time Series Models and Scalable Inference (Matthew James Johnson)
Nice explanation of importance of conjugacy in variational inference
Nice notation using logarithms:

# Variational Bayesian Inference with Stochastic Search
Using stochastic optimization to variational lower bound and uses control variates to decrease variance of the stochastic search gradient.
Using control variates has benefits that used functions doesn’t have to be lower bounds but only correlate with original function.
Directly optimizing variational objective (log likelihood) when some expectations of log joint likelihood (complete likelihood) cannot be computed
Using Monte Carlo for approximating intractable gradient.
Variance reduction approximates function of a random variable by another function which has the same expectation but smaller variance.

# A trust-region method for stochastic variational inference with applications to streaming data
SVI is prone to local optima, sensitive to choice of hyperparameters.
using SVI with natural gradient fails for streaming data but using trust region works.
they use trust region by introducing regularizer that do not allow variational distribution change too much.

# Graphical Models, Exponential Families, and Variational Inference (not finished because too difficult/longg)
variational inference uses variational relaxation (simplifies function being optimized)
Marginalizing we can select order of variables when summing which relates to Fubini’s theorem.
distribution p is consistent with the data if expectations under p for some functions are marched to the expectations under empirical distribution. Then we choose distribution that meets this condition and maximizes entropy. Exponential family meets these criteria.

# Stochastic Variational Inference for Bayesian Time Series Models

Neural Variational Inference and Learning in Belief Networks
Using NN for efficient exact sampling (no mixing issues like convergence and correlated samples like MCMCM) from the variational posterior of the Sigmoid Belief Network
 Technique applicable for the online setting and much faster than MCMC
 “The model and this inference network are trained jointly by maximizing a variational lower bound on the log-likelihood”
 They use also variance reduction methods for the gradient computed
Can handle continuous and discrete latent variables.
Training an inference model by following the gradient of the variational bound.
Better results than wake sleep algorithm
This approach does not use any local variational parameters instead they use feed forward model to compute variational distribution from the observation –inference network. No architecture specific approximations.
We compute gradients of the lower bound separately wrt model parameters and wrt approximating model parameters. The involve expectations wrt Q. We sample from Q using inference network and compute MC estimates.
Equation 8 is incorrect, we should have \grad_psi 1 ? Question
Can be compared to black box VI which is sampling based.
NVIL optimizes a variational lower bound on the log likelihood but wake sleep algorithm does not optimize well defined objective function.
Can use inference networks with autoregressive connections within each layer.
Use lower bound on log likelihood as model performance metric.

# Auto-Encoding Variational Bayes
2014
Continuous latent variables.
Efficient approximation of posterior distribution and marginal likelihood when EM or VB are not easy. For example for neural networks.
Intractability and large data sets
Can be used for representational learning.
In contrast to mean field VI the recognition model is not necessarily factorial and its parameters are not computed from some closed form expectionation. 
We can treat recognition model as probabilistic encoder for the data. The p(x|z) is probabilistic decoder. Probabilistic encoder because given x data we output probability over z (later representation)
The naive MC estimator of the gradient of ELBO with respect to variational parameters exhibits high variance.
Using reparametrization trick to decrease the variance of MC estimator
reparametrization trick also used because we cannot differentiate through sampling process (expectation wrt to q and cannot differentiate wrt its parameters). Explained at talk: https://www.youtube.com/watch?v=rjZL7aguLAs
KL divergence between prior and posterior when both are gaussians can be solved analytically
One of future directions pointed out in paper: “time-series models (i.e. dynamic Bayesian networks)”
This model assumes i.i.d. data and my data is not i.i.d.
Description how to build marginal likelihood estimator using HMCMC
It shows marginal likelihood lower bound in two shapes:

nice reviews http://openreview.net/document/94ac4bf7-6122-449a-90af-0ac47e98dda0
The regularizer term which keep the approximate posterior and prior of latent variables close turns off excessive latent variables. So we can create big model and the unnecessary latent capacity will be switched off automatically.
Nice explanation why simple latent represenation can model complicated distribution: https://ift6266h15.files.wordpress.com/2015/04/20_vae.pdf (VAE lecture.pdf).
By adding the noise we restrict the amount of information we pass through latent layer (explained https://www.youtube.com/watch?v=P78QYjWh5sM ). When using continuous latent variables we can have infinite amount of information so we have to restrict it by adding noise. The different data points when mapped to similar latent variables has to be at least different that amount of noise so we can reconstruct them back to different values. In this lecture Karol Gregor shows how this idea relates to information theory (coding length)

# Deep AutoRegressive Networks (not read)
The same as “Auto-Encoding Variational Bayes” but binary version of z with autoregressive p(z) 
In contrast to models that generate correlated samples from previous samples the DARN model produces uncorrelated samples.
“compressed representations are good for making predictions and hence also good for generating samples”
Deriving lower bound from MDL perspective
here we optimise parameters of q(H|X) and p(X,H) simultaneously while variational learning optimise the parameters by co-ordinate descent.

# Streaming Variational Bayes (not read)

Pylearn2: a machine learning research library
research library
built on top of mathematical expression compiler, Theano
DenseDesignMatrixPyTables used to represent data which cannot fit into memory. It uses HDF5 format.
Dataset contains preprocessing like ZCA and PCA so worth exploring it.
The Model class is not required to know how to train itself, though many models do (there should be training algorithms that are separate from the models).
Spaces now how to convert between them. Different linear operators expect data formatted in different ways.

# What's Wrong with Variational Inference
Variational inference often produces worse parameter estimates than MCMC.
local optima introduced by the mean-field approximation.
Structured Stochastic Variational Inference helps (removing assumption about factorial approximate posterior and introduce dependencies)

# An Introduction to Variational Methods for Graphical Models
Representing neural network as sigmoid belief net. The inference is intractable because during moralization all the parents become connected.
RESEARCH IDEA: Training neural networks as probabilistic methods using approximate methods like variational methods.
Variational method can be used to aproximate posterior and intractable energy term and enable tractable inference and learning.
Variational methods transform local probabilities so the inference becomes tractable and possible we can use exact methods.

#[Stochastic Backpropagation and Approximate Inference in Deep Generative Models]()
2014

The same idea as “Auto-Encoding Variational Bayes”
Expressing gradient of expectation of function as expectation of gradient of a function (example: Gaussian gradient identities). 
Stochastic backpropagation as in Auto-Encoding Variational Bayes paper and using amortization. The model is deep model which latent variables 
is the gaussian + tranformation of previous stochastic layer. 

# [Variational inference with normalizing flows](https://arxiv.org/abs/1505.05770)
2015.05.15

Authors notice the we can improve variational methods by using richer approximate posteriors by using normalizing
flows i.e. series of invertible mappings of one random variable into another. They conjecture that this approach is the universal approximator
of any posterior distribution. They use amortized version to train the Deep Gaussian Latent Model on CFAIR data and custom toy densities and compare
to previous methods.

#[Auxiliary Deep Generative Models]()
2016

Another way of improving variation methods is to use more expressive variational distributions by adding auxiliary latent 
variables. This enables more expressive approximate distributions even if we are using factorized Gaussian for each factor: 
$q(\mathbf{z},\mathbf{a}|\mathbf{x})= q(\mathbf{z}|\mathbf{a},\mathbf{x})q(\mathbf{a}|\mathbf{x})$.
Authors show how to perform semisupervised learning in this kind of models.

#[Improved variational inference with inverse autoregressive flow]()
2016

Uses flow similar to inverse of autoregressive transformation to create flow for estimating high dimensional probability function which
can be used for variational posterior approximation. They use MADE to create autoregressive mpas which output affine parameters of the
invertible transforms. The have simple determinant jackobian so the created normalizing flow is simple to compute. The first parmaeters
are encoded using normal neural network and the next use autoregressive MADE.

#[Variational Inference: A Review for Statisticians]()
2017
Review of the variational methods with some derivations.

# [Learnable explicit density for continuous latent space and variational inference]()
demonstrates general universal representational capability of inverse autoregressive transformations
Learning better prior allows to model better approximate posterior. Here we have a proof that the sequence of autoregressive
 mappings can approximate any continuous distributions. Read it later.


#[An Introduction to Variational Autoencoders]()
2019

Overview of VAE and its improvements, applications, literature. Applying other models to improve VAE performance like
 Normalizing Flows. Very thorough and clear.