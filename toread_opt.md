No more pesky learning rates
Adaptive subgradient methods for online learning and stochastic optimization
Adam: A method for stochastic optimization
Numerical Methods for Unconstrained Optimization and Nonlinear Equations
Second-Order Stochastic Optimization for Machine Learning in Linear Time
Local gain adaptation in stochastic gradient descent
Combining conjugate direction methods with stochastic approximation of gradients
Large-scale machine learning with stochastic gradient descent
Tieleman, T. & Hinton, G. RmsProp: divide the gradient by a running average of its recent magnitude. Lecture 6.5 of Neural Networks for Machine Learning(COURSERA, 2012); available at http://www.cs.toronto.edu/~ tijmen/csc321/slides/lecture_slides_lec6.pdf
Gradient-based hyperparameter optimization through reversible learning