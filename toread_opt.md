2000 - A survey of truncated-Newton methods
2013 - No more pesky learning rates
2016 - On large-batch training for deep learning: Generalization gap and sharp minima
2017 - A bayesian perspective on generalization and stochastic gradient descent
2017 - Train longer, generalize better: closing the generalization gap in large batch training of neural networks
2019 - Adaptive Gradient Methods with Dynamic Bound of Learning Rate
    (https://github.com/taki0112/AdaBound-Tensorflow, 
    https://github.com/kozistr/AdaBound-tensorflow)

Adaptive subgradient methods for online learning and stochastic optimization
Adam: A method for stochastic optimization
Numerical Methods for Unconstrained Optimization and Nonlinear Equations
Second-Order Stochastic Optimization for Machine Learning in Linear Time
Local gain adaptation in stochastic gradient descent
Combining conjugate direction methods with stochastic approximation of gradients
Large-scale machine learning with stochastic gradient descent
Tieleman, T. & Hinton, G. RmsProp: divide the gradient by a running average of its recent magnitude. Lecture 6.5 of Neural Networks for Machine Learning(COURSERA, 2012); available at http://www.cs.toronto.edu/~ tijmen/csc321/slides/lecture_slides_lec6.pdf
Gradient-based hyperparameter optimization through reversible learning
Online Convex Programming and Generalized Infinitesimal Gradient Ascent
Learning Gradient Descent: Better Generalization and Longer Horizons - 2017
YellowFin and the Art of Momentum Tuning - 2017
Decoupled Weight Decay Regularization - 2017