1960 - Adaptive sampled-data systems
1987 - Geometric analysis of neural network capabilities
1989 - Approximation by superpositions of a sigmoidal function
1989 - Pattern classification using neural networks
1990 - Learning from hints in neural networks
1993 - Second order derivatives for network pruning: Optimal Brain Surgeon
2009 - Quadratic features and deep architectures for chunking
2011 - Deep sparse rectifier neural networks
2014 - Bi-modal derivative activation function for sigmoidal feedforward networks
2014 - Bi-firing deep neural networks
2014 - Hardware implementation of evolvable block-based neural networks utilizing a cost efficient sigmoid-like activation function
2014 - On practical constraints of approximation using neural networks on current digital computers
2015 - Highway Networks
2015 - All you need is a good init
2016 - Effective deep learning-based multi-modal retrieval
2016 - Deep Residual Learning for Image Recognition
2016 - https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b
2016 - Distributed B-SDLM: Accelerating the Training Convergence of Deep Neural Networks Through Parallelism
2016 - https://tebesu.github.io/posts/Training-a-TensorFlow-graph-in-C++-API
2017 - Softsign as a Neural Networks Activation Function
2017 - Axiomatic Attribution for Deep Networks
2018 - Analysis on Gradient Propagation in Batch Normalized Residual Networks
2018 - Regularisation of neural networks by enforcing Lipschitz continuity
2019 - Hands-On Computer Vision with TensorFlow 2
    https://github.com/PacktPublishing/Hands-On-Computer-Vision-with-TensorFlow-2
 
Learning internal representations by error propagation
Approximations by superpositions of sigmoidal functions
Approximation capabilities of multilayer feedforward networks
Approximation with Artificial Neural Networks
Universal approximation of an unknown mapping and its derivatives using multilayer feedforward networks
Degree of approximation results for feedforward networks approximating unknown mappings and their derivatives
Networks with trainable amplitude of activation functions
Polynomial Regression As an Alternative to Neural Nets
Neural clouds for monitoring of complex systems
Generalized Constraint Neural Network Regression Model Subject to Linear Priors
Hints and the VC Dimension
Efficient Pattern Recognition Using a New Transformation Distance
Financial Market Applications of Learning from Hints Neural Networks in the Capital Markets
Knowledge Incorporation into Neural Networks From Fuzzy Rules
Application of MLP networks to bond rating and house pricing
Calibrating artificial neural networks by global optimization   
https://deepmind.com/blog/population-based-training-neural-networks/
Practical Recommendations for Gradient-Based Training of Deep Architectures
Learning Inductive Biases with Simple Neural Networks
Rectified linear units improve restricted Boltzmann machines
Improving deep neural networks for LVCSR using rectified linear units and dropout
Deep learning via Hessian-free optimization
Extracting and composing robust features with denoising autoencoders
Saturating auto-encoders
Scalable Bayesian optimization using deep neural networks
Learning representations by back-propagating errors
Deep learning with limited numerical precision
Applied Optimal Control: Optimization, Estimation, and Control <- one of the original backpropagation
Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences <- one of the original backpropagation
Learning-logic: Casting the cortex of the human brain in silicon
Deep neuroevolution: Genetic algorithms are a competitive alternative for training deep neural networks for reinforcement learning
A memoryless BFGS neural network training algorithm
Regularization tools for training large feed-forward neural networks using automatic differentiation
Application of PID controller based on BP neural network using automatic differentiation method
Optimization of neural network feedback control systems using automatic differentiation
Caffe con troll: Shallow ideas to speed up deep learning
cuDNN: Efficient primitives for deep learning
Binaryconnect: Training deep neural networks with binary weights during propagations
Differentiable optimization as a layer in neural networks
Transformation invariance in pattern recognition, tangent distance and tangent propagation
Perceprrons
Neural Machine Translation by Jointly Learning to Align and Translate
A learned representation for artistic style
Empirical evaluation of rectified activations in convolutional network
Fast and accurate deep network learning by exponential linear units (elus)
Hypernetworks
Learning feed-forward one-shot learners
Dynamic filter networks
Why Should I Trust You Explaining the Predictions of Any Classifier
https://www.oreilly.com/learning/introduction-to-local-interpretable-model-agnostic-explanations-lime
https://www.kaggle.com/emanceau/interpreting-machine-learning-lime-explainer
https://github.com/marcotcr/lime
Learning in linear networks: a survey
Neural Network Design and the Complexity of Learning
The loading problem for pyramidal neural networks
Loading deep networks is hard
Loading deep networks is hard: The pyramidal case
Graphical models: foundations of neural computation
Sumâ€“product networks: A new deep architecture
Leveraging hierarchical parametric networks for skeletal joints based action segmentation and recognition
Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion
Parallel networks that learn to pronounce English text
Scaling relationships in backpropagation learning
What size net gives valid generalization
On the complexity of polyhedral separability
Perceptrons An Introduction to Computational Geometry
Don't Decay the Learning Rate, Increase the Batch Size
On the Convergence of Adam and Beyond

model/cdf_comb_1/xy_prefinal_l_0_1/summaries_model/cdf_comb/xy_prefinal_l_0/weights-raw_0/model/cdf_comb/xy_prefinal_l_0/weights-raw_0_hist
model/cdf_comb_1/xy_prefinal_l_0/summaries_model/cdf_comb/xy_prefinal_l_0/weights-raw_0/model/cdf_comb/xy_prefinal_l_0/weights-raw_0_hist

Very Deep Convolutional Networks for Large-Scale Visual Recognition
Deeply-Supervised Nets
Resnet in resnet: Generalizing residual architectures
Sharp minima can generalize for deep nets
Keeping the neural networks simple by minimizing the description length of the weights
Densely connected convolutional networks
Identity mappings in deep residual networks