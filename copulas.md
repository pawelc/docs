#[Fonctions de repartition a n dimensions et leurs marges]()
1959

Sklar's paper.

#[A probabilistic interpretation of complete monotonicity]()
1974 

I think it is original paper about Archimedean copulas but too difficult for me to understand all the proofs.

#[Families of multivariate distributions]()
1988

Paper introducing archimedean copulas as Laplace transformation of positive distribution function: p(x>0)=1.

#[Copulas for Finance A Reading Guide and Some Applications]()
2000

Overview of different properties of the copula functions like concordance, dependence. Also formulas for all basic 
copulas. For example there are given different equations for empirical copula. The cite interesting facts that fitting
copula model together with univariate margins using ML gives different parameters than using Inference Functions for 
Margins (where we fit margins and copula separately). There is also CML method (Canonical Maximum Likelihood)
where we transform variables via their respective empirical distributions and then we fit copula to this data.
Authors gives different examples of copula application to financial problems but I haven't read it because
most of them were on stochastic processes. There were some for risk management which are might worth reading.

#[An Introduction to Copulas]()
2007

Comprehensive book about copulas

#[Copula Bayesian Networks]()
2010

Authors create model that introduces copula approach into graphical models. This enables us to model univariate marginals
separately and local terms using copula function. Authors use the same methods for inference and training as in 
standard BN (Bayesian Networks). The use problems of larger dimensionality then in other copula papers so probably 
good to reuse them with our model. They parametrize conditional density functions using copulas. Each copula use to 
construct each factor can be different. To make learning efficient authors estimate marginals first using standard
normal kernel-based approach. The local copulas can be estimated separately and they use 2 simple copulas: Frank and
Gaussian Copula. The optimization of parameters is done using ML. Authors say that estimating multivariate Gaussian 
copula is more efficient by using relationship between copula function and Kendall's Tau dependence measure. The structure
is also learnt using score-based approach.

#[Copulas in machine learning]()
2013

Tree structured graphical models which has each bivariate dependence encoded as copula. If more
general  structure is needed we can use mixture over such trees.


# [Deep Archimedean Copulas]()
2020

Authors propose method of composing Archimedean copula completely monotone generators using Deep Network approximator.
They use previously known theorem that a generator phi is completely monotone if and only if phi is the Laplace 
transform of a positive random variable M. There is theorem allowing to interpret copula generated by this phi in terms
of this M variable. This gives very simple algorithm to sample from C when we can sample from M.
Therefore we can construct generators as finite sum/mixture of negative exponentials.
The network take convex combination of the previous layer followed by multiplication by the negative exponential. 
Because sum of exponentials is closed under addition and multiplication of sums of exponentials, the result of the 
network remains convex combination of negative exponentials. To compute the inverse of the generator they use Newton's
root finding method. Authors notice that probabilistic queries that can be answered by distribution models are outside
of simple application of neural probabilistic model like GANs, Normalizing Flows. The queries like conditional 
probabilities. The model can also learn on uncertain data (data which gives ranges instead of single values) which
can be achieved easily if we have distribution function.
Experiments in the paper: learning from samples generating from popular Archimedean copulas, learning from simple
real world data-sets (authors flip the variables to model negative correlations). They use the data sets we used
in the PUMONDE like GAS, POWER.
Authors to not fit marginals in the model but only pre-process the data via the empirical marginal distributions to model
only copula.
Authors do not recommend using the ACNet model to large dimensions because of the numerical problems with multiple 
differentiation of the copula function and because of the highly dimensional data being rarely symmetric. Our model
should better deal with in-symmetries of the data and also modeling higher dimensions easier with composite likelihood.
There are some interesting examples of data not accurately modelled by the ACNet like POWER data set because of 
high level of discreteness (few appliances using fixed amount of electricity).

# [Generative Archimedean Copulas]()
2021

Here authors propose method that should work for higher dimensions. They propose mixture model with hidden variables. 
They parametrize latent distribution which Laplace transform acts as the generator function. Author say that this
construction allows scaling computations to higher dimensions (using properties of the Laplace transform to compute
higher order derivatives) and bypass numerical problems with automatic differentiation. They simply use properties
of nice form of the multiple derivative of the exponential function. This model scales linearly with dimensionality
to compute density where "Deep Archimedean Copulas" scales exponentially.  
To compute the generator they need to sample
to compute Laplace integral (they sample from generative network that samples positive variables M).
They also use hierarchical composition of Archimedean copulas to make them more expressive (for
example asymmetrical). Using Laplace transform on learnt latent model also provides means of sampling from the copula.
The difference between this model and "Deep Archimedean Copulas" is that the previous estimator uses NN to construct
generator and this work uses NN to build generative model of hidden variable and Laplace transform is empirical.
Authors use 3 methods of training: maximum likelihood, goodness-of-fit (distance from the empirical copula) 
and adversarial training. When using maximum likelihood they sill need to inert the generator and 
they do it by the iterative Newton method. The sampling technique does not require differentiation of the copula 
distribution nor the inversion of the conditional distribution.
They use the same experiments as in "Deep Archimedean Copulas".
Still have to understand hierarchical construction because have to understand generation using compound Poisson process.
Additionally they run experiments for larger dimensionality.
In one of the experiments they show that the hierarchical version of the model can learn asymmetries in the multidimensional
data (4 dimensions). They show nicely the samples from ground truth and learnt model using once scatter matrix.