#[Fonctions de repartition a n dimensions et leurs marges]()
1959

Sklar's paper.

#[A probabilistic interpretation of complete monotonicity]()
1974 

I think it is original paper about Archimedean copulas but too difficult for me to understand all the proofs.

#[Families of multivariate distributions]()
1988

Paper introducing archimedean copulas as Laplace transformation of positive distribution function: p(x>0)=1.

#[Copulas and markov processes]()
1992

Here authors define special product of copulas, defined as integral of product of partial derivatives of 2 copulas, where
we integrate over not differentiated variable. Not read entire paper because quite long.

#[Copulas for Finance A Reading Guide and Some Applications]()
2000

Overview of different properties of the copula functions like concordance, dependence. Also formulas for all basic 
copulas. For example there are given different equations for empirical copula. The cite interesting facts that fitting
copula model together with univariate margins using ML gives different parameters than using Inference Functions for 
Margins (where we fit margins and copula separately). There is also CML method (Canonical Maximum Likelihood)
where we transform variables via their respective empirical distributions and then we fit copula to this data.
Authors gives different examples of copula application to financial problems but I haven't read it because
most of them were on stochastic processes. There were some for risk management which are might worth reading.

#[An Introduction to Copulas]()
2007

Comprehensive book about copulas

#[Enjoy the Joy of Copulas: With a Package copula]()
2007

The overview of R package of copulas. Authors have section discussing numerical problems when implementing copulas
as software library.

#[Copula Bayesian Networks]()
2010

Authors create model that introduces copula approach into graphical models. This enables us to model univariate marginals
separately and local terms using copula function. Authors use the same methods for inference and training as in 
standard BN (Bayesian Networks). The use problems of larger dimensionality then in other copula papers so probably 
good to reuse them with our model. They parametrize conditional density functions using copulas. Each copula use to 
construct each factor can be different. To make learning efficient authors estimate marginals first using standard
normal kernel-based approach. The local copulas can be estimated separately and they use 2 simple copulas: Frank and
Gaussian Copula (uniform correlation and with full correlation matrix). 
The optimization of parameters is done using ML. Authors say that estimating multivariate Gaussian 
copula is more efficient by using relationship between copula function and Kendall's Tau dependence measure. The structure
is also learnt using score-based approach by adding 1 parent at a time starting from 0 parents therefore learning only marginals
and seeing additional benefit of modelling dependency when parents are added. 
Authors compare their models in terms of log-probability to Bayesian Networks.
The datasets used: UCI (Wine Quality-12 variables, Crime-100 observed variables), Dow Jones (28 variables). CBN
gives better results than BN. It looks when the structure is learnt adding more than 1 parent does not improve
performance much, although the improvement is more visible for more complex domains. Authors also check how dense networks
are learnt and it looks CBN learn sparser networks also achieving better log-probability than BN.
Authors also show samples generated by learnt models on bivariate scatter plots. In this domain CDN also excels.
Authors emphasize that their tackle problem with 100 dimensions where in the literature problems tackled are < 10 variables.

[Likelihood inference for Archimedean copulas in high dimensions under known margins]()
2012

Authors derive analytical formulas for derivatives of various generators for archimedean copulas. They are needed to 
compute density of the copula, evaluation the kendal distribution function, conditional distribution functions. 
Authors notice that the computation of derivatives needs for maximum likelihood estimation is doable but prone 
to numerical problems even when using high precision. Thanks to computing the derivatives analytically authors can 
tackle maximum likelihood estimation for dimensions in range of 10-100.
Authors report that computing high order of derivatives of the generator (50,100) can lead to numerical/computational problems
like results taking too long to compute (aborted computation after 10 minutes), returning wrong value, returning different
value each time. Actually I tried to compute the 50th derivative of the Gumbel generator using jax and it failed ... it
uses all my memory and then dies so potentially more memory I have available is needed (I have 50 GB)
Did not read it fully because got to difficult.

#[Copulas in machine learning]()
2013

Tree structured graphical models which has each bivariate dependence encoded as copula. If more
general  structure is needed we can use mixture over such trees.

#[Model-based clustering using copulas with applications]()
2014

Model is built using copula functions as components of mixture. Authors notice that this model is more flexible
then using regular mixture of standard elliptical distributions or their skewed versions. Usually we would need
much more components of the mixture then real clusters when using elliptical distributions. Using copulas we can
model any marginal and we can use continuous and discrete distributions. Authors use EM algorithm to train the model.  
The algorithm is started
from clusters recovered by the k-medoids algorithm. Authors fit different order of copulas because they give different
local optima. The clusters recovered by the copula mixture is much better were it succeeded in discovering correct
cluster shapes. Authors cluster also data for NBA players using copula mixture approach with each margin using 
adequate parametric distribution family like Gamma or Beta. The copula use is Gaussian. They also use rotated copulas
especially survival copulas i.e. regular copulas rotated by 180 degrees. So we can rotate copulas by 90,180 or 270 degrees.
Depending on the rotation we achieve change of dependence sign or able to model upper or lower tail dependence (in
copulas that model for example only upper tail dependence and positive dependence).
I guess we can look as competitor to our model where this is mixture of experts and ours is product of experts.
Also they look at bivariate density model which uses rotated variable by angle in (0,2pi] and each mixture component
can have different rotation. They do this in the density space. This way they may choose the same copula family for
each component. When we use elliptical copula there are identifybility issues for the roatation parameter. The same issue
happens for the Archimedean copulas when their parameter makes variables intependent.
Here also we have correct gaussian copula analytical formula given, as I noticed we have wrong in our paper. 
The mixture copulas are easy to marginalize because elliptical, archimedean copulas are closed under marginalization.
Authors mention the rotation of copulas is possible in higher dimensions but the number of parameters can become
impractical. The point to the work on latent angular processes for the rotation angles which use only few parameters.
For the large dimensional problems one can use vice copulas (product of marginal densities and bivariate copula densities) 
but this approach does not allow for easy marginalization and numerical integration is necessary.
 

#[Pair-Copula Bayesian Networks]()
2014 

Authors combine paired copula construction with Bayesian Networks encoding conditional independence. Have to read first
about vein copulas and paired networks before reading this paper.


# [Deep Archimedean Copulas]()
2020

Authors propose method of composing Archimedean copula completely monotone generators using Deep Network approximator.
They use previously known theorem that a generator phi is completely monotone if and only if phi is the Laplace 
transform of a positive random variable M. There is theorem allowing to interpret copula generated by this phi in terms
of this M variable. This gives very simple algorithm to sample from C when we can sample from M.
Therefore we can construct generators as finite sum/mixture of negative exponentials.
The network take convex combination of the previous layer followed by multiplication by the negative exponential. 
Because sum of exponentials is closed under addition and multiplication of sums of exponentials, the result of the 
network remains convex combination of negative exponentials. To compute the inverse of the generator they use Newton's
root finding method. Authors notice that probabilistic queries that can be answered by distribution models are outside
of simple application of neural probabilistic model like GANs, Normalizing Flows. The queries like conditional 
probabilities. The model can also learn on uncertain data (data which gives ranges instead of single values) which
can be achieved easily if we have distribution function.
Experiments in the paper: learning from samples generating from popular Archimedean copulas, learning from simple
real world data-sets (authors flip the variables to model negative correlations). They use the data sets we used
in the PUMONDE like GAS, POWER.
Authors to not fit marginals in the model but only pre-process the data via the empirical marginal distributions to model
only copula.
Authors do not recommend using the ACNet model to large dimensions because of the numerical problems with multiple 
differentiation of the copula function and because of the highly dimensional data being rarely symmetric. Our model
should better deal with in-symmetries of the data and also modeling higher dimensions easier with composite likelihood.
There are some interesting examples of data not accurately modelled by the ACNet like POWER data set because of 
high level of discreteness (few appliances using fixed amount of electricity).

#[copulas in risk management]()
2020

Authors use Mathematica to compute VaR using Monte Carlo methods modeling dependence with copulas. Authors give overview
of simple copulas. The portfolio for which VaR is computed is composed of 2 indices: S&P 500 and NASDAQ. The risk
factors are daily log returns. In the end they found the Gaussian copula was the best for this task.

#[A bayesian semiparametric archimedean copula]()
2020 

The generator proposed in this paper builds the copula that can achieve kendal tau (-1,1). Authors propose semi/nonparametric
model based on survival analysis idea. They model derivative of the hazard rate as the piecewise constant function
and from there they compute: hazard function, cumulative hazard and survival function (which come out as piecewise
quadratic function) which is the inverse of the generator. Authors train the model in Bayesian way.

#[Copula-Based Assessment of Co-Movement and Tail Dependence Structure Among Major Trading Foreign Currencies in Ghana]()
2020

Authors model Ghana's currency with respect to USD,GBP and EUR using copula approach. The margins are modeled using
ARMA + GARCH models. The copulas used are Gaussian, t, Gumbel/Clayton and rotated version of the Gumbel/Clayton.
The correlation parameter of the Gaussian and t is modeled as ARMA (1,q) process. The archimedean copulas are estimated 
using Generalized Autoregressive Score (GAS). Authors present results for time invariant copula estimation and time 
varying copulas. For the model selection they use AIC criteria and various statistical test to check goodness of fit.
Overall the t-copula was the best fit. Authors compare such copula model to the multivariate GARCH model with
dynamic conditional correlation (DCC-GARCH model). Authors conclude that used data (exchange rates) needs models that 
can capture tail dependence, time varying dependence and volatility.

# [Generative Archimedean Copulas]()
2021

Here authors propose method that should work for higher dimensions. They propose mixture model with hidden variables. 
They parametrize latent distribution which Laplace transform acts as the generator function. Author say that this
construction allows scaling computations to higher dimensions (using properties of the Laplace transform to compute
higher order derivatives) and bypass numerical problems with automatic differentiation. They simply use properties
of nice form of the multiple derivative of the exponential function. This model scales linearly with dimensionality
to compute density where "Deep Archimedean Copulas" scales exponentially.  
To compute the generator they need to sample
to compute Laplace integral (they sample from generative network that samples positive variables M).
They also use hierarchical composition of Archimedean copulas to make them more expressive (for
example asymmetrical). Using Laplace transform on learnt latent model also provides means of sampling from the copula.
The difference between this model and "Deep Archimedean Copulas" is that the previous estimator uses NN to construct
generator and this work uses NN to build generative model of hidden variable and Laplace transform is empirical.
Authors use 3 methods of training: maximum likelihood, goodness-of-fit (distance from the empirical copula) 
and adversarial training. When using maximum likelihood they sill need to inert the generator and 
they do it by the iterative Newton method. The sampling technique does not require differentiation of the copula 
distribution nor the inversion of the conditional distribution.
They use the same experiments as in "Deep Archimedean Copulas".
Still have to understand hierarchical construction because have to understand generation using compound Poisson process.
Additionally they run experiments for larger dimensionality.
In one of the experiments they show that the hierarchical version of the model can learn asymmetries in the multidimensional
data (4 dimensions). They show nicely the samples from ground truth and learnt model using once scatter matrix.

# [Copula flows for synthetic data generation]()
2021 

Authors use normalizing flows to model marginals and copulas. They have copula flow (inverse copula distribution) and 
marginal flows (inverse of marginal distributions). 
They use it to generate synthetic data sets. The present 
the data with 2 rings and they say that none of the copulas can learn it but their model can. So I can use it for our model
as well. Their model can estimate mixed data i.e. discrete and continuous. They give nice example when you learn copula with marginals
and then swap one of the marginals for a different distribution like age for a different country.
Authors use distributional transform that create integral transform for discrete data. Besides benchmarking its 
models on log probability (checking the marginal performance + copula performance) they also use some standard benchmark
where data is generated from the model and on this data other ML model is learnt. This way we can check if model can really 
create valid samples. They also say the log probability is not good metric of goodness of fit so they use 
Kolmogorov-Smirnov (KS) test for goodness of fit.