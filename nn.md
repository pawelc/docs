# A statistical view of deep learning 
DNN = Recursive Generalized Linear Models
Autoencoders as Variational approach to learning generative models.
RNN as state space models with deterministic dynamics.
Maximum likelihood, MAP and overfitting. But they lack transformation invariance.
Thoughts when model is deep.

# A neural network is a monference, not a model
NN as model with inference. Shows that RNN directly recovere inference of HMM (filtering) with the same precision. The same as in 
"Probabilistic Interpretations of Recurrent Neural Networks"

# Uncertainty in Deep Learning
Mapping VI with Stochastic Regularization Techniques like dropout. Should  be implementable for DNC.
There are a lot of references in this work worth reading. It would be nice to read later to the end especially about usage 