The Kanerva Machine: A Generative Distributed Memory
What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision
Learning Stochastic Recurrent Networks
Variational Recurrent Auto-Encoders
DRAW: A Recurrent Neural Network For Image Generation
Learning visual motion in recurrent neural networks
Modeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation and transcription
Bayesian Data Analysis Straight-line Fitting
Neural GPUs Learn Algorithms
Learning Wake-Sleep Recurrent Attention Models
Learning Structured Output Representation using Deep Conditional Generative Models
An Uncertain Future: Forecasting from Static Images using Variational Autoencoders
Backpropagation through time: what it does and how to do it
Learning to Predict by the Methods of Temporal Differences
Evolving Neural Turing Machines for Reward-based Learning
Scaling Memory-Augmented Neural Networks with Sparse Reads and Writes
Can Active Memory Replace Attention
Pointer networks
Neural Machine Translation by Jointly Learning to Align and Translate
Neural Variational Inference for Text Processing
Hard attention:    
Show, Attend and Tell: Neural Image Caption Generation with Visual Attention
LIE-ACCESS NEURAL TURING MACHINES
Neural programmer: Inducing latent programs with gradient descent
Neural GPUs
Learning to Remember Rare Events
Learning to Transduce with Unbounded Memory
Evolutionary multi-objective generation of recurrent neural network ensembles for time series prediction
The Goldilocks Principle Reading Children's Books with Explicit Memory Representations
Towards ai-complete question answering a set of prerequisite toy tasks
Recurrent memory network for language modeling
Learning longer memory in recurrent neural networks
Large-scale Simple Question Answering with Memory Networks
Evaluating Prerequisite Qualities for Learning End-to-End Dialog Systems
Automatic Rule Extraction from Long Short Term Memory Networks
Neural Map Structured Memory for Deep Reinforcement Learning
Condensed Memory Networks for Clinical Diagnostic Inferencing
Frustratingly Short Attention Spans in Neural Language Modeling
Gated End-to-End Memory Networks
Dynamic Key-Value Memory Networks for Knowledge Tracing
Lie Access Neural Turing Machine
Attentive Memory Networks Efficient Machine Reading for Conversational Search
Recurrent Memory Addressing for describing videos
Dynamic Neural Turing Machine with Continuous and Discrete Addressing Schemes
Learning Operations on a Stack with Neural Turing Machines
Deep Multi-Task Learning with Shared Memory
A Theory of Sequence Indexing and Working Memory in Recurrent Neural Networks
Recurrent Memory Array Structures
Improving the Neural GPU Architecture for Algorithm Learning
Extensions and Limitations of the Neural GPU
Learning Efficient Algorithms with Hierarchical Attentive Memory
Reasoning with Memory Augmented Neural Networks for Language Comprehension
Deep Networks with Internal Selective Attention through Feedback Connections
Learning phrase representations using rnn encoder-decoder for statistical machine translation
A Multi-World Approach to Question Answering about Real-World Scenes based on Uncertain Input
Learning a recurrent visual representation for image caption generation
Stacked attention networks for image question answering
Teaching machines to read and comprehend
Attention and Augmented Recurrent Neural Networks https://distill.pub/2016/augmented-rnns
Match memory recurrent networks
Going deeper: Autonomous steering with neural memory networks
Memory-Augmented Neural Networks for Predictive Process Analytics
Gated Feedback Recurrent Neural Networks
Bayesian Recurrent Neural Networks
Recursive Bayesian Recurrent Neural Networks for Time-Series Modeling
https://en.wikipedia.org/wiki/Differentiable_neural_computer
https://www.reddit.com/r/MachineLearning/comments/5ebmdr/discussion_uncertainty_propagation_in_lstmbased/
https://openreview.net/forum?id=BkDB51WR-
Lstm recurrent networks learn simple context-free and context-sensitive languages
Memory Augmented Neural Networks with Wormhole Connections
Feedforward Sequential Memory Neural Networks without Recurrent Feedback
Cumulative distribution networks
Cumulative distribution networks and the derivative-sum-product algorithm

Self-Delimiting Neural Networks
Optimal ordered problem solver
Applications of quantum inspired computational intelligence: a survey
Deep Learning for Time-Series Analysis
One-Shot Generalization in Deep Generative Models
One-shot Learning with Memory-Augmented Neural Networks
Cortical microcircuits as gated-recurrent neural networks
Learning to Diagnose with LSTM Recurrent Neural Networks
Recurrent generative adversarial networks for proximal learning and automated compressive image recovery
Generalization of deep neural networks for chest pathology classification in x-rays using generative adversarial networks
Speech recognition with missing data using recurrent neural nets
Modeling Missing Data in Clinical Time Series with RNNs
Doctor AI: Predicting Clinical Events via Recurrent Neural Networks
DeepCare A Deep Dynamic Memory Model for Predictive Medicine
Directly Modeling Missing Data in Sequences with RNNs Improved Classification of Clinical Time Series
Differential recurrent neural networks for action recognition
https://machinelearningmastery.com/promise-recurrent-neural-networks-time-series-forecasting/
Supervised learning from incomplete data via an EM approach
Visualizing and Understanding Recurrent Networks (also video https://skillsmatter.com/skillscasts/6611-visualizing-and-understanding-recurrent-networks)
Framewise phoneme classiﬁcation with bidirectional LSTM and other neural network architectures
Training Recurrent Networks by Evolino
Evolving memory cell structures for sequence learning
Long Short-Term Memory Recurrent Neural Network Architectures for Large Scale Acoustic Modeling
An Empirical Exploration of Recurrent Network Architectures 
Long Short-Term Memory in Recurrent Neural Networks
LSTM can Solve Hard Long Time Lag Problems
Gradient Flow in Recurrent Nets the Difficulty of Learning Long-Term Dependencies
Stock Price Prediction via Discovering Multi-Frequency Trading Patterns
Accelerating Recurrent Neural Networks A Memory-Efficient Approach
Learning compact recurrent neural networks
Unsupervised Learning in Recurrent Neural Networks
Real-time interactive sequence generation and control with Recurrent Neural Network ensembles
From Bayesian Sparsity to Gated Recurrent Nets
Recurrent Additive Networks
Mixture density networks for distribution and uncertainty estimation
Long-term Recurrent Convolutional Networks for Visual Recognition and Description
Translating videos to natural language using deep recurrent neural networks
Video description generation incorporating spatiotemporal features and a soft-attention mechanism
Sequence to Sequence -- Video to Text
Recurrent Models of Visual Attention
Multiple Object Recognition with Visual Attention
https://deepmind.com/blog/differentiable-neural-computers/
https://www.youtube.com/watch?v=steioHoiEms
https://github.com/deepmind/dnc
https://www.youtube.com/watch?v=QuvRWevJMZ4 - attention
Generative Adversarial Networks
Domain-Adversarial Neural Networks
Domain-Adversarial Training of Neural Networks
Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation (this is about GRU)
Empirical evaluation of gated recurrent neural networks on sequence modeling
Depth-Gated LSTM
A clockwork RNN
A Long Short-Term Memory Recurrent Neural Network Framework for Network Traffic Matrix Prediction
Structural-RNN: Deep Learning on Spatio-Temporal Graphs
Adaptive Computation Time for Recurrent Neural Networks
Spatially Adaptive Computation Time for Residual Networks (https://www.youtube.com/watch?v=xp5lLiA-hA8)
Training recurrent networks online without backtracking
A tutorial on training recurrent neural networks, covering BPPT, RTRL, EKF and the "echo state network" approach
Decoupled Neural Interfaces using Synthetic Gradients
Curriculum Learning
Active Learning Literature Survey
Unifying Count-Based Exploration and Intrinsic Motivation
Automated Curriculum Learning for Neural Networks
VIME Variational Information Maximizing Exploration
https://github.com/salesforce/pytorch-qrnn
Gradient-Based Learning Algorithms for Recurrent Networks and Their Computational Complexity
http://people.idsia.ch/~juergen/rnn.html
Learning to Control Fast-Weight Memories An Alternative to Dynamic Recurrent Networks
Extraction of rules from discrete-time recurrent neural networks
Deep learning in neural networks An overview
Learning Context Free Grammars Limitations of a Recurrent Neural Network with an External Stack Memory
A Connectionist Symbol Manipulator That Discovers the Structure of Context-Free Languages
Learning to coBayesian Recurrent Neural Networks
Recursive Bayesian Recurrent Neural Networks for Time-Series Modelingntrol fast-weight memories: An alternative to recurrent nets
Learning Precise Timing with LSTM Recurrent Networks
An introspective network that can learn to run its own weight change algorithm
Learning To Learn Using Gradient Descent
Learning to learn by gradient descent by gradient descent
http://people.idsia.ch/~juergen/metalearner.html
Sequential Constant Size Compressors for Reinforcement Learning
Sequence Labelling in Structured Domains with Hierarchical Recurrent Neural Networks
Learning Complex, Extended Sequences Using the Principle of History Compression
Evolving Neural Networks in Compressed Weight Space
https://github.com/openai/multiagent-competition
https://blog.openai.com/competitive-self-play/
http://people.idsia.ch/~juergen/optimalsearch.html
http://people.idsia.ch/~juergen/evolino.html
Driven by Compression Progress A Simple Principle Explains Essential Aspects of Subjective Beauty, Novelty, Surprise, Interestingness, Attention, Curiosity, Creativity, Art, Science, Music, Jokes
Resource-Bounded Machines are Motivated to be Effective, Efficient, and Curious
https://aeon.co/essays/how-close-are-we-to-creating-artificial-intelligence
https://www.icsi.berkeley.edu/icsi/events/2014/08/schmidhuber-rnn
https://blog.openai.com/competitive-self-play/
https://github.com/openai/multiagent-competition
https://r2rt.com/styles-of-truncated-backpropagation.html
Statistical Language Models based on Neural Networks
On the difficulty of training recurrent neural networks
On Multiplicative Integration with Recurrent Neural Networks
Layer Normalization
https://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html
Deep Residual Learning for Image Recognition
Highway Networks
Learning Long-Term Dependencies with Gradient Descent is Difficult.
Protein Secondary Structure Prediction with Long Short Term Memory Networks
Random search for hyper-parameter optimization
An Efficient Approach for Assessing Hyperparameter Importance
Generalized Functional ANOVA Diagnostics for High-Dimensional Functions of Dependent Variables
Recurrent Highway Networks (https://github.com/julian121266/RecurrentHighwayNetworks)
Highway and Residual Networks learn Unrolled Iterative Estimation
DropIn: Making Reservoir Computing Neural Networks Robust to Missing Inputs by Dropout
Multi-directional Recurrent Neural Networks A Novel Method for Estimating Missing Data
Explaining Recurrent Neural Network Predictions in Sentiment Analysis
E-RNN Entangled Recurrent Neural Networks for Causal Prediction
Recurrent neural network based language model
Context dependent recurrent neural network language model
Deep Computational Phenotyping
An overview and comparative analysis of Recurrent Neural Networks for Short Term Load Forecasting
Predicting Remaining Useful Life using Time Series Embeddings based on Recurrent Neural Networks
m-TSNE: A Framework for Visualizing High-Dimensional Multivariate Time Series
Variational Recurrent Adversarial Deep Domain Adaptation
Grounded Recurrent Neural Networks
Second-order training for recurrent neural networks without teacher-forcing
Modeling Progression Free Survival in Breast Cancer with Tensorized Recurrent Neural Networks and Accelerated Failure Time Models
End-to-end attention-based large vocabulary speech recognition
Attention-Based Models for Speech Recognition
Sequence to Sequence Learning with Neural Networks
A note on the evaluation of generative models
Mind’s Eye: A Recurrent Visual Representation for Image Caption Generation
Time Series for Macroeconomics and Finance
Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks
An Actor-Critic Algorithm for Sequence Prediction
Search-based structured prediction
A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning
How (not) to Train your Generative Model: Scheduled Sampling, Likelihood, Adversary?
The neural autoregressive distribution estimator
Zoneout Regularizing RNNs by Randomly Preserving Hidden Activations
Recurrent neural networks with limited numerical precision
Batch normalized recurrent neural networks
Attention is all you need
Banishing the homunculus: making working memory work
Continuous attractors and oculomotor control
Expressive power of recurrent neural networks
https://arxiv.org/find/cs/1/au:+Fulcher_B/0/1/0/all/0/1
https://github.com/openai/blocksparse
Distilling a Neural Network Into a Soft Decision Tree
Bengio, Y., Louradour, J., Collobert, R. & Weston, J. Curriculum learning
Overcoming catastrophic forgetting in neural networks
Memory-based neural networks for robot learning
Prediction with a Short Memory
Can recurrent neural networks warp time?
Building machines that learn and think like people
Matching Networks for One Shot Learning
Learning Simple Algorithms from Examples
Meta-Learning with Memory-Augmented Neural Networks
Feedforward Sequential Memory Networks: A New Structure to Learn Long-term Dependency
Hierarchical Memory Networks
Memory-augmented Neural Machine Translation
LSTM with working memory
Learning to Generate with Memory
Active One-shot Learning
Position-based Content Attention for Time Series Forecasting with Sequence-to-sequence RNNs
Self-paced learning for latent variable models
Delving into adversarial attacks on deep policies
Kalman filters improve lstm network performance in problems unsolvable by traditional recurrent nets
Sequential Neural Models with Stochastic Layers
Memory-Efficient Backpropagation Through Time
The Future of Memory Remembering, Imagining, and the Brain
Is the brain a good model for machine intelligence
Evidence integration in model-based tree search
Artificial intelligence Deep neural reasoning
Kalman filtering and neural networks
Training recurrent networks using the extended kalman filter
An evolutionary algorithm that constructs recurrent neural networks
Evolutionary optimization of long short-term memory neural network language model
Long Short-Term Memory Over Tree Structures
Differential recurrent neural networks for action recognition
Semantic object parsing with local-global long short-term memory
Lstm time and frequency recurrence for automatic speech recognition
Convolutional lstm network: A machine learning approach for precipitation nowcasting
Unitary evolution recurrent neural networks
Gated Orthogonal Recurrent Units: On Learning to Forget
Improving neural networks with dropout
Rnndrop: A novel dropout for rnns in asr
Recurrent dropout without memory loss
Regularizing rnns by stabilizing activations
PRNN Recurrent Neural Network with Persistent Memory
Cascade Dynamics Modeling with Attention-based Recurrent Neural Network
Attention-based Mixture Density Recurrent Networks for History-based Recommendation
a learning algorithm for continually running fully recurrent neural networks
Tieleman, T. & Hinton, G. RmsProp: divide the gradient by a running average of its recent magnitude. Lecture 6.5 of Neural Networks for Machine Learning(COURSERA, 2012); available at http://www.cs.toronto.edu/~ tijmen/csc321/slides/lecture_slides_lec6.pdf
Nonlinear Statistical Models
In All Likelihood: Statistical Modelling and Inference Using Likelihood
Concrete problems in ai safety
Time-series Extreme Event Forecasting with Neural Networks at Uber
A neural probabilistic language model
Distilling the Knowledge in a Neural Network
Neural Variational Inference and Learning in Belief Networks
Inserting rules into recurrent neural networks
Rule refinement with recurrent neural networks
On Learning to Think Algorithmic Information Theory for Novel Combinations of Reinforcement Learning Controllers and Recurrent Neural World Models
The time dimension of neural network models
Learning Sequence Representations
Learning Algorithms from Data
First-pass large vocabulary continuous speech recognition using bi-directional recurrent dnns
Recurrent Ladder Networks
Recurrent continuous translation models
Recurrent Memory Networks for Language Modeling
Multiplicative LSTM for sequence modelling
Optimizing Performance of Recurrent Neural Networks on GPUs
Interpretable Structure-Evolving LSTM
Neural Program Lattices
Improved TDNNs using Deep Kernels and Frequency Dependent Grid-RNNs
Nested LSTMs
Deep Neural Architectures for Algorithms and Sequential Data
Quasi-Recurrent Neural Networks
Mechanisms for sentence processing
Context-free parsing in connectionist networks
A recurrent network that performs a context-sensitive prediction task
A recurrent neural network that learns to count
Toward a connectionist model of recursion in human linguistic performance
Fractal encoding of context-free grammars in connectionist networks
Context-free and context-sensitive dynamics in recurrent neural networks
Generalization of backpropagation with application to a recurrent gas market model
https://www.basicbooks.com/titles/judea-pearl/the-book-of-why/9780465097609/
https://www.quantamagazine.org/to-build-truly-intelligent-machines-teach-them-cause-and-effect-20180515/
https://medium.com/applied-data-science/how-to-build-your-own-alphazero-ai-using-python-and-keras-7f664945c188
http://colah.github.io/posts/2015-08-Understanding-LSTMs/
https://medium.com/@aidangomez/the-neural-turing-machine-79f6e806c0a1
https://blog.aidangomez.ca/2016/04/17/Backpropogating-an-LSTM-A-Numerical-Example/
https://www.reddit.com/r/MachineLearning/comments/4xnuv2/what_is_the_general_belief_on_value_of_neural/
https://www.reddit.com/r/MachineLearning/comments/64lkry/d_explanation_of_deepminds_neural_turing_machine/
https://discuss.pytorch.org/t/neural-turing-machines-ntm-implementation/8276
https://www.quora.com/What-is-a-neural-Turing-machine
https://summerofcode.withgoogle.com/archive/2017/projects/6637623116300288/
https://stats.stackexchange.com/questions/241286/how-are-controllers-attached-to-read-write-heads-in-neural-turing-machines
https://trollheaven.wordpress.com/2017/12/15/neural-turing-machine/
A Novel Fractional Gradient-Based Learning Algorithm for Recurrent Neural Networks
Tree Memory Networks for Modelling Long-term Temporal Dependencies
On the memory properties of recurrent neural models
Divide and Conquer Networks
Neural Programmer-Interpreters
DeepCoder: Learning to Write Programs
Recurrent Neural Network for Text Classification with Multi-Task Learning
Tracking the World State with Recurrent Entity Networks
Deep generative stochastic networks trainable by backprop