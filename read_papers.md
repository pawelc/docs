# NRAM 
the circuits are like mini algorithms (subroutine) that are executed step by step. More difficult problems are hard to learn possibly because of weak optimization. Possibly QA can help.
# ADDING GRADIENT NOISE IMPROVES LEARNING FOR VERY DEEP NETWORKS
this uses annealed noise added to gradient. Maybe it can be compared to QA. Added gradient noise is similar to simulated annealing. Adding noise only helps with very deep architectures h > 5. 
For simple FFNN It looks that using good init and grad clipping achieves the same results as the same with grad noise. Grad noise helps when only simple init. But it is still useful when network is complex and we are not sure about proper init.
But this is not true for more complex architectures where finding perfect init values is much harder.
# LIE-ACCESS NEURAL TURING MACHINES
# Neural Programmer
# Neural GPUs
